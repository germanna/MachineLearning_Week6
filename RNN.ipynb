{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1jNMeVM89fl",
        "outputId": "634131da-0780-44c0-8f97-e77c9d841b64"
      },
      "source": [
        "!pip install d2l"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting d2l\n",
            "  Downloading d2l-0.17.0-py3-none-any.whl (83 kB)\n",
            "\u001b[?25l\r\u001b[K     |████                            | 10 kB 25.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 20 kB 29.4 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 30 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 40 kB 19.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 51 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 61 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 71 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 81 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 83 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from d2l) (1.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from d2l) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from d2l) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from d2l) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from d2l) (2.23.0)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (5.2.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (7.6.5)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (5.3.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (4.10.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (5.1.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (5.6.1)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l) (5.3.5)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l) (5.1.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (0.8.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter->d2l) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter->d2l) (0.2.5)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l) (3.5.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l) (0.2.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l) (1.0.2)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l) (5.1.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->d2l) (4.8.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->d2l) (2.6.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (0.12.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (1.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (2.11.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->d2l) (22.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->d2l) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->d2l) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l) (1.3.2)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (0.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (1.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (4.1.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (0.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (0.7.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l) (21.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->d2l) (2018.9)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l) (1.11.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->d2l) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->d2l) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->d2l) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->d2l) (2021.5.30)\n",
            "Installing collected packages: d2l\n",
            "Successfully installed d2l-0.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrsS-5VjPeAU"
      },
      "source": [
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "from d2l import tensorflow as d2l\n",
        "\n",
        "\n",
        "T = 1000  # Generate a total of 1000 points\n",
        "time = tf.range(1, T + 1, dtype=tf.float32)\n",
        "x = tf.sin(0.01 * time) + tf.random.normal([T], 0, 0.2)\n",
        "d2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnfkRV-9P0Lm"
      },
      "source": [
        "tau = 4\n",
        "features = tf.Variable(tf.zeros((T - tau, tau)))\n",
        "for i in range(tau):\n",
        "    features[:, i].assign(x[i:T - tau + i])\n",
        "labels = tf.reshape(x[tau:], (-1, 1))\n",
        "\n",
        "batch_size, n_train = 16, 600\n",
        "# Only the first `n_train` examples are used for training\n",
        "train_iter = d2l.load_array((features[:n_train], labels[:n_train]),\n",
        "                            batch_size, is_train=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrWPigEFP5jR"
      },
      "source": [
        "# Vanilla MLP architecture\n",
        "def get_net():\n",
        "    net = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(10, activation='relu'),\n",
        "        tf.keras.layers.Dense(1)])\n",
        "    return net\n",
        "\n",
        "# Least mean squares loss\n",
        "# Note: L2 Loss = 1/2 * MSE Loss. TensorFlow has MSE Loss that is slightly\n",
        "# different from MXNet's L2Loss by a factor of 2. Hence we halve the loss\n",
        "# value to get L2Loss in TF\n",
        "loss = tf.keras.losses.MeanSquaredError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WHqkkK3QM-4",
        "outputId": "f148086a-9819-42ec-b6c4-aa92e3352ee4"
      },
      "source": [
        "def train(net, train_iter, loss, epochs, lr):\n",
        "    trainer = tf.keras.optimizers.Adam()\n",
        "    for epoch in range(epochs):\n",
        "        for X, y in train_iter:\n",
        "            with tf.GradientTape() as g:\n",
        "                out = net(X)\n",
        "                l = loss(y, out) / 2\n",
        "                params = net.trainable_variables\n",
        "                grads = g.gradient(l, params)\n",
        "            trainer.apply_gradients(zip(grads, params))\n",
        "        print(f'epoch {epoch + 1}, '\n",
        "              f'loss: {d2l.evaluate_loss(net, train_iter, loss):f}')\n",
        "\n",
        "net = get_net()\n",
        "train(net, train_iter, loss, 5, 0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, loss: 0.357611\n",
            "epoch 2, loss: 0.249772\n",
            "epoch 3, loss: 0.165225\n",
            "epoch 4, loss: 0.103898\n",
            "epoch 5, loss: 0.073350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tIdKWEtQPZN"
      },
      "source": [
        "onestep_preds = net(features)\n",
        "d2l.plot([time, time[tau:]], [x.numpy(), onestep_preds.numpy()], 'time', 'x',\n",
        "         legend=['data', '1-step preds'], xlim=[1, 1000], figsize=(6, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmuPNFmkQTOO"
      },
      "source": [
        "multistep_preds = tf.Variable(tf.zeros(T))\n",
        "multistep_preds[:n_train + tau].assign(x[:n_train + tau])\n",
        "for i in range(n_train + tau, T):\n",
        "    multistep_preds[i].assign(\n",
        "        tf.reshape(net(tf.reshape(multistep_preds[i - tau:i], (1, -1))), ()))\n",
        "\n",
        "d2l.plot([time, time[tau:], time[n_train + tau:]], [\n",
        "    x.numpy(),\n",
        "    onestep_preds.numpy(), multistep_preds[n_train + tau:].numpy()], 'time',\n",
        "         'x', legend=['data', '1-step preds',\n",
        "                      'multistep preds'], xlim=[1, 1000], figsize=(6, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZXq-Q1YQXUi"
      },
      "source": [
        "max_steps = 64\n",
        "\n",
        "features = tf.Variable(tf.zeros((T - tau - max_steps + 1, tau + max_steps)))\n",
        "# Column `i` (`i` < `tau`) are observations from `x` for time steps from\n",
        "# `i + 1` to `i + T - tau - max_steps + 1`\n",
        "for i in range(tau):\n",
        "    features[:, i].assign(x[i:i + T - tau - max_steps + 1].numpy())\n",
        "\n",
        "# Column `i` (`i` >= `tau`) are the (`i - tau + 1`)-step-ahead predictions for\n",
        "# time steps from `i + 1` to `i + T - tau - max_steps + 1`\n",
        "for i in range(tau, tau + max_steps):\n",
        "    features[:, i].assign(tf.reshape(net((features[:, i - tau:i])), -1))\n",
        "\n",
        "steps = (1, 4, 16, 64)\n",
        "d2l.plot([time[tau + i - 1:T - max_steps + i] for i in steps],\n",
        "         [features[:, (tau + i - 1)].numpy() for i in steps], 'time', 'x',\n",
        "         legend=[f'{i}-step preds'\n",
        "                 for i in steps], xlim=[5, 1000], figsize=(6, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20CNcylFQanK"
      },
      "source": [
        "#2. Text Preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY6SfJ9SQi_H"
      },
      "source": [
        "import collections\n",
        "import re\n",
        "from d2l import tensorflow as d2l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-b7SBITQnIR"
      },
      "source": [
        "#reading dataset\n",
        "#@save\n",
        "d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',\n",
        "                                '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
        "\n",
        "def read_time_machine():  #@save\n",
        "    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n",
        "    with open(d2l.download('time_machine'), 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
        "\n",
        "lines = read_time_machine()\n",
        "print(f'# text lines: {len(lines)}')\n",
        "print(lines[0])\n",
        "print(lines[10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51qLFewPQ0ST"
      },
      "source": [
        "#Tokenization\n",
        "def tokenize(lines, token='word'):  #@save\n",
        "    \"\"\"Split text lines into word or character tokens.\"\"\"\n",
        "    if token == 'word':\n",
        "        return [line.split() for line in lines]\n",
        "    elif token == 'char':\n",
        "        return [list(line) for line in lines]\n",
        "    else:\n",
        "        print('ERROR: unknown token type: ' + token)\n",
        "\n",
        "tokens = tokenize(lines)\n",
        "for i in range(11):\n",
        "    print(tokens[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52aDtWejQ6-I"
      },
      "source": [
        "#Vocabulary\n",
        "class Vocab:  #@save\n",
        "    \"\"\"Vocabulary for text.\"\"\"\n",
        "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
        "        if tokens is None:\n",
        "            tokens = []\n",
        "        if reserved_tokens is None:\n",
        "            reserved_tokens = []\n",
        "        # Sort according to frequencies\n",
        "        counter = count_corpus(tokens)\n",
        "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
        "                                   reverse=True)\n",
        "        # The index for the unknown token is 0\n",
        "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
        "        self.token_to_idx = {\n",
        "            token: idx for idx, token in enumerate(self.idx_to_token)}\n",
        "        for token, freq in self._token_freqs:\n",
        "            if freq < min_freq:\n",
        "                break\n",
        "            if token not in self.token_to_idx:\n",
        "                self.idx_to_token.append(token)\n",
        "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "    def to_tokens(self, indices):\n",
        "        if not isinstance(indices, (list, tuple)):\n",
        "            return self.idx_to_token[indices]\n",
        "        return [self.idx_to_token[index] for index in indices]\n",
        "\n",
        "    @property\n",
        "    def unk(self):  # Index for the unknown token\n",
        "        return 0\n",
        "\n",
        "    @property\n",
        "    def token_freqs(self):  # Index for the unknown token\n",
        "        return self._token_freqs\n",
        "\n",
        "def count_corpus(tokens):  #@save\n",
        "    \"\"\"Count token frequencies.\"\"\"\n",
        "    # Here `tokens` is a 1D list or 2D list\n",
        "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
        "        # Flatten a list of token lists into a list of tokens\n",
        "        tokens = [token for line in tokens for token in line]\n",
        "    return collections.Counter(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a5xqQxzRJsL"
      },
      "source": [
        "vocab = Vocab(tokens)\n",
        "print(list(vocab.token_to_idx.items())[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccRnGBLhRNBJ"
      },
      "source": [
        "for i in [0, 10]:\n",
        "    print('words:', tokens[i])\n",
        "    print('indices:', vocab[tokens[i]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5avSrXtRPzB"
      },
      "source": [
        "def load_corpus_time_machine(max_tokens=-1):  #@save\n",
        "    \"\"\"Return token indices and the vocabulary of the time machine dataset.\"\"\"\n",
        "    lines = read_time_machine()\n",
        "    tokens = tokenize(lines, 'char')\n",
        "    vocab = Vocab(tokens)\n",
        "    # Since each text line in the time machine dataset is not necessarily a\n",
        "    # sentence or a paragraph, flatten all the text lines into a single list\n",
        "    corpus = [vocab[token] for line in tokens for token in line]\n",
        "    if max_tokens > 0:\n",
        "        corpus = corpus[:max_tokens]\n",
        "    return corpus, vocab\n",
        "\n",
        "corpus, vocab = load_corpus_time_machine()\n",
        "len(corpus), len(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GF6ZMunRTBB"
      },
      "source": [
        "#3. Natural Language Statistic\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXwdn11-Rhp1"
      },
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "from d2l import tensorflow as d2l\n",
        "\n",
        "tokens = d2l.tokenize(d2l.read_time_machine())\n",
        "# Since each text line is not necessarily a sentence or a paragraph, we\n",
        "# concatenate all text lines\n",
        "corpus = [token for line in tokens for token in line]\n",
        "vocab = d2l.Vocab(corpus)\n",
        "vocab.token_freqs[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG3oNPdZRk9h"
      },
      "source": [
        "freqs = [freq for token, freq in vocab.token_freqs]\n",
        "d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)', xscale='log',\n",
        "         yscale='log')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu8fV9vKRoQ-"
      },
      "source": [
        "bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]\n",
        "bigram_vocab = d2l.Vocab(bigram_tokens)\n",
        "bigram_vocab.token_freqs[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaJKJvsDRrSd"
      },
      "source": [
        "trigram_tokens = [\n",
        "    triple for triple in zip(corpus[:-2], corpus[1:-1], corpus[2:])]\n",
        "trigram_vocab = d2l.Vocab(trigram_tokens)\n",
        "trigram_vocab.token_freqs[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtAcDDjwRugZ"
      },
      "source": [
        "bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]\n",
        "trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]\n",
        "d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',\n",
        "         ylabel='frequency: n(x)', xscale='log', yscale='log',\n",
        "         legend=['unigram', 'bigram', 'trigram'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBcGDoGiRxnm"
      },
      "source": [
        "#random sampling\n",
        "def seq_data_iter_random(corpus, batch_size, num_steps):  #@save\n",
        "    \"\"\"Generate a minibatch of subsequences using random sampling.\"\"\"\n",
        "    # Start with a random offset (inclusive of `num_steps - 1`) to partition a\n",
        "    # sequence\n",
        "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
        "    # Subtract 1 since we need to account for labels\n",
        "    num_subseqs = (len(corpus) - 1) // num_steps\n",
        "    # The starting indices for subsequences of length `num_steps`\n",
        "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
        "    # In random sampling, the subsequences from two adjacent random\n",
        "    # minibatches during iteration are not necessarily adjacent on the\n",
        "    # original sequence\n",
        "    random.shuffle(initial_indices)\n",
        "\n",
        "    def data(pos):\n",
        "        # Return a sequence of length `num_steps` starting from `pos`\n",
        "        return corpus[pos:pos + num_steps]\n",
        "\n",
        "    num_batches = num_subseqs // batch_size\n",
        "    for i in range(0, batch_size * num_batches, batch_size):\n",
        "        # Here, `initial_indices` contains randomized starting indices for\n",
        "        # subsequences\n",
        "        initial_indices_per_batch = initial_indices[i:i + batch_size]\n",
        "        X = [data(j) for j in initial_indices_per_batch]\n",
        "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
        "        yield tf.constant(X), tf.constant(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddPB1rNOSBAG"
      },
      "source": [
        "my_seq = list(range(35))\n",
        "for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):\n",
        "    print('X: ', X, '\\nY:', Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCnuttXaSEZu"
      },
      "source": [
        "def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save\n",
        "    \"\"\"Generate a minibatch of subsequences using sequential partitioning.\"\"\"\n",
        "    # Start with a random offset to partition a sequence\n",
        "    offset = random.randint(0, num_steps)\n",
        "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
        "    Xs = tf.constant(corpus[offset:offset + num_tokens])\n",
        "    Ys = tf.constant(corpus[offset + 1:offset + 1 + num_tokens])\n",
        "    Xs = tf.reshape(Xs, (batch_size, -1))\n",
        "    Ys = tf.reshape(Ys, (batch_size, -1))\n",
        "    num_batches = Xs.shape[1] // num_steps\n",
        "    for i in range(0, num_batches * num_steps, num_steps):\n",
        "        X = Xs[:, i:i + num_steps]\n",
        "        Y = Ys[:, i:i + num_steps]\n",
        "        yield X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqhl1U7JSHaA"
      },
      "source": [
        "for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):\n",
        "    print('X: ', X, '\\nY:', Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hsh79fOrSKmT"
      },
      "source": [
        "class SeqDataLoader:  #@save\n",
        "    \"\"\"An iterator to load sequence data.\"\"\"\n",
        "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
        "        if use_random_iter:\n",
        "            self.data_iter_fn = d2l.seq_data_iter_random\n",
        "        else:\n",
        "            self.data_iter_fn = d2l.seq_data_iter_sequential\n",
        "        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)\n",
        "        self.batch_size, self.num_steps = batch_size, num_steps\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8-ZmpimSNZC"
      },
      "source": [
        "def load_data_time_machine(batch_size, num_steps,  #@save\n",
        "                           use_random_iter=False, max_tokens=10000):\n",
        "    \"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\n",
        "    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter,\n",
        "                              max_tokens)\n",
        "    return data_iter, data_iter.vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_QXeLzSyF6"
      },
      "source": [
        "#4. RNN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URjZs0mXSzv0"
      },
      "source": [
        "import tensorflow as tf\n",
        "from d2l import tensorflow as d2l\n",
        "\n",
        "X, W_xh = tf.random.normal((3, 1), 0, 1), tf.random.normal((1, 4), 0, 1)\n",
        "H, W_hh = tf.random.normal((3, 4), 0, 1), tf.random.normal((4, 4), 0, 1)\n",
        "tf.matmul(X, W_xh) + tf.matmul(H, W_hh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdCzhAzAS25d"
      },
      "source": [
        "tf.matmul(tf.concat((X, H), 1), tf.concat((W_xh, W_hh), 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ygiWpNxS6K-"
      },
      "source": [
        "#5.Implementation of Recurrent Neural Networks from Scratch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJKYOlaNTC7i"
      },
      "source": [
        "%matplotlib inline\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from d2l import tensorflow as d2l\n",
        "\n",
        "\n",
        "batch_size, num_steps = 32, 35\n",
        "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co9n4OG-TIt1"
      },
      "source": [
        "tf.one_hot(tf.constant([0, 2]), len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brChu2zbTMIG"
      },
      "source": [
        "X = tf.reshape(tf.range(10), (2, 5))\n",
        "tf.one_hot(tf.transpose(X), 28).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekWDbRS8TO6-"
      },
      "source": [
        "def get_params(vocab_size, num_hiddens):\n",
        "    num_inputs = num_outputs = vocab_size\n",
        "\n",
        "    def normal(shape):\n",
        "        return tf.random.normal(shape=shape, stddev=0.01, mean=0,\n",
        "                                dtype=tf.float32)\n",
        "\n",
        "    # Hidden layer parameters\n",
        "    W_xh = tf.Variable(normal((num_inputs, num_hiddens)), dtype=tf.float32)\n",
        "    W_hh = tf.Variable(normal((num_hiddens, num_hiddens)), dtype=tf.float32)\n",
        "    b_h = tf.Variable(tf.zeros(num_hiddens), dtype=tf.float32)\n",
        "    # Output layer parameters\n",
        "    W_hq = tf.Variable(normal((num_hiddens, num_outputs)), dtype=tf.float32)\n",
        "    b_q = tf.Variable(tf.zeros(num_outputs), dtype=tf.float32)\n",
        "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUzDX3GtTRfn"
      },
      "source": [
        "def init_rnn_state(batch_size, num_hiddens):\n",
        "    return (tf.zeros((batch_size, num_hiddens)),)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzejmKIzTUCZ"
      },
      "source": [
        "def rnn(inputs, state, params):\n",
        "    # Here `inputs` shape: (`num_steps`, `batch_size`, `vocab_size`)\n",
        "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
        "    H, = state\n",
        "    outputs = []\n",
        "    # Shape of `X`: (`batch_size`, `vocab_size`)\n",
        "    for X in inputs:\n",
        "        X = tf.reshape(X, [-1, W_xh.shape[0]])\n",
        "        H = tf.tanh(tf.matmul(X, W_xh) + tf.matmul(H, W_hh) + b_h)\n",
        "        Y = tf.matmul(H, W_hq) + b_q\n",
        "        outputs.append(Y)\n",
        "    return tf.concat(outputs, axis=0), (H,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DcUyIusTW6R"
      },
      "source": [
        "class RNNModelScratch:  #@save\n",
        "    \"\"\"A RNN Model implemented from scratch.\"\"\"\n",
        "    def __init__(self, vocab_size, num_hiddens, init_state, forward_fn,\n",
        "                 get_params):\n",
        "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
        "        self.init_state, self.forward_fn = init_state, forward_fn\n",
        "        self.trainable_variables = get_params(vocab_size, num_hiddens)\n",
        "\n",
        "    def __call__(self, X, state):\n",
        "        X = tf.one_hot(tf.transpose(X), self.vocab_size)\n",
        "        X = tf.cast(X, tf.float32)\n",
        "        return self.forward_fn(X, state, self.trainable_variables)\n",
        "\n",
        "    def begin_state(self, batch_size, *args, **kwargs):\n",
        "        return self.init_state(batch_size, self.num_hiddens)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "jEAkTTMtTaUE",
        "outputId": "2a470798-8d8c-4fe9-ccc7-742f9e020ee1"
      },
      "source": [
        "# defining tensorflow training strategy\n",
        "device_name = d2l.try_gpu()._device_name\n",
        "strategy = tf.distribute.OneDeviceStrategy(device_name)\n",
        "\n",
        "num_hiddens = 512\n",
        "with strategy.scope():\n",
        "    net = RNNModelScratch(len(vocab), num_hiddens, init_rnn_state, rnn,\n",
        "                          get_params)\n",
        "state = net.begin_state(X.shape[0])\n",
        "Y, new_state = net(X, state)\n",
        "Y.shape, len(new_state), new_state[0].shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-6f66e78642e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# defining tensorflow training strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdevice_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtry_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOneDeviceStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_hiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'd2l' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUlZhuEPTecW"
      },
      "source": [
        "#prediction\n",
        "def predict_ch8(prefix, num_preds, net, vocab):  #@save\n",
        "    \"\"\"Generate new characters following the `prefix`.\"\"\"\n",
        "    state = net.begin_state(batch_size=1, dtype=tf.float32)\n",
        "    outputs = [vocab[prefix[0]]]\n",
        "    get_input = lambda: tf.reshape(tf.constant([outputs[-1]]), (1, 1)).numpy()\n",
        "    for y in prefix[1:]:  # Warm-up period\n",
        "        _, state = net(get_input(), state)\n",
        "        outputs.append(vocab[y])\n",
        "    for _ in range(num_preds):  # Predict `num_preds` steps\n",
        "        y, state = net(get_input(), state)\n",
        "        outputs.append(int(y.numpy().argmax(axis=1).reshape(1)))\n",
        "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRrsG_07Tj_T"
      },
      "source": [
        "predict_ch8('time traveller ', 10, net, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm-thWj9TnhF"
      },
      "source": [
        "def grad_clipping(grads, theta):  #@save\n",
        "    \"\"\"Clip the gradient.\"\"\"\n",
        "    theta = tf.constant(theta, dtype=tf.float32)\n",
        "    new_grad = []\n",
        "    for grad in grads:\n",
        "        if isinstance(grad, tf.IndexedSlices):\n",
        "            new_grad.append(tf.convert_to_tensor(grad))\n",
        "        else:\n",
        "            new_grad.append(grad)\n",
        "    norm = tf.math.sqrt(\n",
        "        sum((tf.reduce_sum(grad**2)).numpy() for grad in new_grad))\n",
        "    norm = tf.cast(norm, tf.float32)\n",
        "    if tf.greater(norm, theta):\n",
        "        for i, grad in enumerate(new_grad):\n",
        "            new_grad[i] = grad * theta / norm\n",
        "    else:\n",
        "        new_grad = new_grad\n",
        "    return new_grad"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvB9-aQpTrM4"
      },
      "source": [
        "#training\n",
        "#@save\n",
        "def train_epoch_ch8(net, train_iter, loss, updater, use_random_iter):\n",
        "    \"\"\"Train a model within one epoch (defined in Chapter 8).\"\"\"\n",
        "    state, timer = None, d2l.Timer()\n",
        "    metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\n",
        "    for X, Y in train_iter:\n",
        "        if state is None or use_random_iter:\n",
        "            # Initialize `state` when either it is the first iteration or\n",
        "            # using random sampling\n",
        "            state = net.begin_state(batch_size=X.shape[0], dtype=tf.float32)\n",
        "        with tf.GradientTape(persistent=True) as g:\n",
        "            y_hat, state = net(X, state)\n",
        "            y = tf.reshape(tf.transpose(Y), (-1))\n",
        "            l = loss(y, y_hat)\n",
        "        params = net.trainable_variables\n",
        "        grads = g.gradient(l, params)\n",
        "        grads = grad_clipping(grads, 1)\n",
        "        updater.apply_gradients(zip(grads, params))\n",
        "\n",
        "        # Keras loss by default returns the average loss in a batch\n",
        "        # l_sum = l * float(tf.size(y).numpy()) if isinstance(\n",
        "        #     loss, tf.keras.losses.Loss) else tf.reduce_sum(l)\n",
        "        metric.add(l * tf.size(y).numpy(), tf.size(y).numpy())\n",
        "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpyExrGYTytq"
      },
      "source": [
        "#@save\n",
        "def train_ch8(net, train_iter, vocab, lr, num_epochs, strategy,\n",
        "              use_random_iter=False):\n",
        "    \"\"\"Train a model (defined in Chapter 8).\"\"\"\n",
        "    with strategy.scope():\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "        updater = tf.keras.optimizers.SGD(lr)\n",
        "    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n",
        "                            legend=['train'], xlim=[10, num_epochs])\n",
        "    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab)\n",
        "    # Train and predict\n",
        "    for epoch in range(num_epochs):\n",
        "        ppl, speed = train_epoch_ch8(net, train_iter, loss, updater,\n",
        "                                     use_random_iter)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(predict('time traveller'))\n",
        "            animator.add(epoch + 1, [ppl])\n",
        "    device = d2l.try_gpu()._device_name\n",
        "    print(f'perplexity {ppl:.1f}, {speed:.1f} tokens/sec on {str(device)}')\n",
        "    print(predict('time traveller'))\n",
        "    print(predict('traveller'))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59B6JuhMT3Rm"
      },
      "source": [
        "num_epochs, lr = 500, 1\n",
        "train_ch8(net, train_iter, vocab, lr, num_epochs, strategy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr2NPaEFV2nF"
      },
      "source": [
        "with strategy.scope():\n",
        "    net = RNNModelScratch(len(vocab), num_hiddens, init_rnn_state, rnn,\n",
        "                          get_params)\n",
        "train_ch8(net, train_iter, vocab_random_iter, lr, num_epochs, strategy,\n",
        "          use_random_iter=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}